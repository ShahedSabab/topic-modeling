{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yahoo_answer_top2vec.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7468419428dd418ab3a0ab7d8ab39341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8c1723abd7e4f42b9f18886be3b26c1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9e93731a600448b3b10e5acb6945cf27",
              "IPY_MODEL_0ad678b000cb4c49a12039d159291baf"
            ]
          }
        },
        "d8c1723abd7e4f42b9f18886be3b26c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e93731a600448b3b10e5acb6945cf27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f97bd562d164cd58bb705cf2863f006",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 503702349,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 503702349,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce45a633a8cd431ab71eec0591602661"
          }
        },
        "0ad678b000cb4c49a12039d159291baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88c352531315440caf975bccc3b70681",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 504M/504M [00:22&lt;00:00, 22.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bae06c6f0e1643dc93b6f6188c99c4c4"
          }
        },
        "5f97bd562d164cd58bb705cf2863f006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce45a633a8cd431ab71eec0591602661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88c352531315440caf975bccc3b70681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bae06c6f0e1643dc93b6f6188c99c4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wby5a2C_CuLL"
      },
      "source": [
        "# Install dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-ReFNE5K2BI"
      },
      "source": [
        "# a = []\n",
        "# while(1):\n",
        "#     a.append('1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9BhvxtVCq0J",
        "outputId": "bb5a807d-a998-45bc-de54-1b72997072c9"
      },
      "source": [
        "!pip install top2vec[sentence_encoders]\n",
        "!pip install top2vec[sentence_transformers]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: top2vec[sentence_encoders] in /usr/local/lib/python3.7/dist-packages (1.0.24)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.20.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.1.5)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.5.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.8.27)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.5.1)\n",
            "Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (3.6.0)\n",
            "Requirement already satisfied: tensorflow-text; extra == \"sentence_encoders\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (2.4.3)\n",
            "Requirement already satisfied: tensorflow-hub; extra == \"sentence_encoders\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.11.0)\n",
            "Requirement already satisfied: tensorflow; extra == \"sentence_encoders\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (2.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_encoders]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_encoders]) (2.8.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->top2vec[sentence_encoders]) (7.1.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.22.2.post1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.22)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.51.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->top2vec[sentence_encoders]) (4.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.4.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.36.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.1.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.28.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (4.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.4.1)\n",
            "Requirement already satisfied: top2vec[sentence_transformers] in /usr/local/lib/python3.7/dist-packages (1.0.24)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.1.5)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (0.5.1)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (0.8.27)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.20.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.5.0)\n",
            "Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (3.6.0)\n",
            "Requirement already satisfied: torch; extra == \"sentence_transformers\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.8.1+cu101)\n",
            "Requirement already satisfied: sentence-transformers; extra == \"sentence_transformers\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_transformers]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_transformers]) (2018.9)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.51.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (1.4.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.15.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (0.29.22)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.0.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->top2vec[sentence_transformers]) (7.1.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->top2vec[sentence_transformers]) (4.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.7.4.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.2.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (4.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (0.1.95)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_transformers]) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (0.0.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnxEmZ5aCoxb"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMM3u_fP_-8x",
        "outputId": "01e19f9f-78b0-4fb5-8051-6bf6404af984"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from top2vec import Top2Vec\n",
        "import pickle\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euux3QA3Clh2",
        "outputId": "54f5baec-4b66-4e7d-9fed-ac1924b6bea5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4olB8CBYC7y-"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812ejh__C5Ho"
      },
      "source": [
        "DATASET = 'Dataset-yahoo-answer'\n",
        "PATH = '/content/gdrive/MyDrive/'+DATASET\n",
        "NUM_TOPICS = 20 # used for topic reduction \n",
        "COLUMN = 'answer' # column to use for topic modeling\n",
        "EMBEDDING = 'bert'\n",
        "VERSION = '1.0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvKlAirBDYSm"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8oanoZiDXnm"
      },
      "source": [
        "df_train = pd.read_csv('/content/gdrive/MyDrive/'+DATASET+'/test.csv', header=None ,names=['topic', 'title', 'question', 'answer'])\n",
        "# df_train = pd.read_csv('/content/gdrive/MyDrive/'+DATASET+'eq_bank_training_training_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAElsohmDhPn"
      },
      "source": [
        "# df_train = df_train.groupby(\"parent_id\").sample(frac=0.05, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_pP16E0FhP7v",
        "outputId": "12005a79-8eec-410c-ba54-988e0a2a35ad"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>What makes friendship click?</td>\n",
              "      <td>How does the spark keep going?</td>\n",
              "      <td>good communication is what does it.  Can you m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Why does Zebras have stripes?</td>\n",
              "      <td>What is the purpose or those stripes? Who do t...</td>\n",
              "      <td>this provides camouflage - predator vision is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>What did the itsy bitsy sipder climb up?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>waterspout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>What is the difference between a Bachelors and...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One difference between a Bachelors and a Maste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Why do women get PMS?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Premenstrual syndrome (PMS) is a group of symp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>9</td>\n",
              "      <td>if you could be any fantasy figure, who would ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The invisible man, I'd be straight into the gi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>8</td>\n",
              "      <td>Tell me something about life most people don't...</td>\n",
              "      <td>Do you know anything about life, or words of w...</td>\n",
              "      <td>That there is a hell and everyone thinks their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>3</td>\n",
              "      <td>Why are men always thinking of sex?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's wired in our brain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>6</td>\n",
              "      <td>est ce que DOMENECH est un entraineur: 1: de f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>de foot mais pas pour Ãªtre sÃ©lectionneur d'une...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>5</td>\n",
              "      <td>No sound or low sound?</td>\n",
              "      <td>I have my volume turned up all the way and hav...</td>\n",
              "      <td>As an old techie, I tend to look at the hardwa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       topic  ...                                             answer\n",
              "0          9  ...  good communication is what does it.  Can you m...\n",
              "1          2  ...  this provides camouflage - predator vision is ...\n",
              "2          4  ...                                         waterspout\n",
              "3          4  ...  One difference between a Bachelors and a Maste...\n",
              "4          3  ...  Premenstrual syndrome (PMS) is a group of symp...\n",
              "...      ...  ...                                                ...\n",
              "59995      9  ...  The invisible man, I'd be straight into the gi...\n",
              "59996      8  ...  That there is a hell and everyone thinks their...\n",
              "59997      3  ...                            It's wired in our brain\n",
              "59998      6  ...  de foot mais pas pour Ãªtre sÃ©lectionneur d'une...\n",
              "59999      5  ...  As an old techie, I tend to look at the hardwa...\n",
              "\n",
              "[60000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBsUr5U9DyqP"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO31U3ceDiKi"
      },
      "source": [
        "def preprocess_tokenize(s):\n",
        "    '''\n",
        "    This function takes a string as an input and then preprocess the string.\n",
        "    The preprocessing involves  \n",
        "        1. removing hyperlinks, \n",
        "        2. making all letters lower-case\n",
        "        3. removing all punctuations, special characters and digits\n",
        "        4. tokenization \n",
        "        5. lemmatization\n",
        "    Inputs:\n",
        "        s: s is a string\n",
        "    returns:\n",
        "        processed_string: processed string\n",
        "    '''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        "    \n",
        "    cleanr = re.compile('<.*?>')\n",
        "    s = re.sub(cleanr, '', s)      #removing html tags       \n",
        "    s = re.sub(r\"http\\S+\", \"\", s)  #removing hyperlinks\n",
        "    s = re.sub(r\"www\\S+\", \"\", s)   #removing hyperlinks\n",
        "    s = re.sub(r\"\\\\n\", \"\", s)      #removing \\n \n",
        "    s = s.lower()  \n",
        "    s = tokenizer.tokenize(s)\n",
        "    \n",
        "    processed_string = ''\n",
        "    for word in s:\n",
        "        if word not in stop_words:\n",
        "            word = lemmatizer.lemmatize(word, 'v')\n",
        "            word = lemmatizer.lemmatize(word, 'n')\n",
        "            word = lemmatizer.lemmatize(word, 'a')\n",
        "            processed_string += word\n",
        "            processed_string += ' '\n",
        "            \n",
        "    return processed_string.strip()\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hWb1DQE7D0bI",
        "outputId": "ec0e3b69-d5b6-4478-cb74-20dac72e45bf"
      },
      "source": [
        "# pre - processing \n",
        "df_train['processed_'+COLUMN] = df_train[COLUMN].apply(lambda x: preprocess_tokenize(str(x)))\n",
        "df_train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>processed_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>What makes friendship click?</td>\n",
              "      <td>How does the spark keep going?</td>\n",
              "      <td>good communication is what does it.  Can you m...</td>\n",
              "      <td>good communication move beyond small talk say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Why does Zebras have stripes?</td>\n",
              "      <td>What is the purpose or those stripes? Who do t...</td>\n",
              "      <td>this provides camouflage - predator vision is ...</td>\n",
              "      <td>provide camouflage predator vision usually dif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>What did the itsy bitsy sipder climb up?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>waterspout</td>\n",
              "      <td>waterspout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>What is the difference between a Bachelors and...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One difference between a Bachelors and a Maste...</td>\n",
              "      <td>one difference bachelor master degree requirem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Why do women get PMS?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Premenstrual syndrome (PMS) is a group of symp...</td>\n",
              "      <td>premenstrual syndrome pm group symptom relate ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   topic  ...                                   processed_answer\n",
              "0      9  ...  good communication move beyond small talk say ...\n",
              "1      2  ...  provide camouflage predator vision usually dif...\n",
              "2      4  ...                                         waterspout\n",
              "3      4  ...  one difference bachelor master degree requirem...\n",
              "4      3  ...  premenstrual syndrome pm group symptom relate ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bkzmePK3Ti"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amgHGZTWK5DO"
      },
      "source": [
        "def create_top2vec(texts):\n",
        "    '''\n",
        "    This function train the Top2Vec model\n",
        "    \n",
        "    Inputs:\n",
        "        texts: a list of documents where each document is a string\n",
        "        \n",
        "    Returns:\n",
        "        model: a trained Top2Vec model\n",
        "    '''\n",
        "    if (EMBEDDING == 'universal'):\n",
        "      model = Top2Vec(embedding_model='universal-sentence-encoder', documents = texts)\n",
        "    elif (EMBEDDING == 'bert'):\n",
        "      model = Top2Vec(embedding_model='distiluse-base-multilingual-cased', documents = texts)\n",
        "    else:\n",
        "      model = Top2Vec(embedding_model='universal-sentence-encoder-multilingual', documents = texts)\n",
        "   \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ka6pPFK9zh"
      },
      "source": [
        "def get_doc_topics_reduced(data_df, model):\n",
        "    '''\n",
        "    As Top2Vec can generate amny topics, this function reduces the number of topics generated by Top2Vec\n",
        "    \n",
        "    Inputs:\n",
        "        data_df: pandas dataframe containing all texts\n",
        "        \n",
        "        model: trained Top2Vec model\n",
        "        \n",
        "    Returns:\n",
        "        topic_nums: numpy array of (n,) dimension where n is the number of documents. It contains the topic for corresponding document\n",
        "        \n",
        "        topic_scores: numpy array of (n,) dimension where n is the number of documents. Each entry is the cosine similarity of\n",
        "            the document and topic vector\n",
        "            \n",
        "        topic_words: array of shape(num_topics, 50)\n",
        "            For each topic the top 50 words are returned, in order\n",
        "            of semantic similarity to topic.\n",
        "            \n",
        "        word_scores: array of shape(num_topics, 50)\n",
        "            For each topic the cosine similarity scores of the\n",
        "            top 50 words to the topic are returned.\n",
        "    \n",
        "    '''\n",
        "    doc_idx = np.arange(0, len(data_df))\n",
        "    topic_nums, topic_scores, topic_words, word_scores = model.get_documents_topics(doc_idx, reduced=True)\n",
        "    \n",
        "    return topic_nums, topic_scores, topic_words, word_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk25TqrPD-xm"
      },
      "source": [
        "# applying pre-processing\n",
        "processed_texts = df_train['processed_'+COLUMN].tolist()\n",
        "# without pre - processing\n",
        "# processed_texts = df_train[COLUMN].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "7468419428dd418ab3a0ab7d8ab39341",
            "d8c1723abd7e4f42b9f18886be3b26c1",
            "9e93731a600448b3b10e5acb6945cf27",
            "0ad678b000cb4c49a12039d159291baf",
            "5f97bd562d164cd58bb705cf2863f006",
            "ce45a633a8cd431ab71eec0591602661",
            "88c352531315440caf975bccc3b70681",
            "bae06c6f0e1643dc93b6f6188c99c4c4"
          ]
        },
        "id": "z48HvKBkLRvP",
        "outputId": "c24fc87c-850a-4c98-8385-aece969119c6"
      },
      "source": [
        "model = create_top2vec(processed_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-09 01:09:11,084 - top2vec - INFO - Pre-processing documents for training\n",
            "2021-04-09 01:09:19,435 - top2vec - INFO - Downloading distiluse-base-multilingual-cased model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7468419428dd418ab3a0ab7d8ab39341",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=503702349.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-09 01:09:52,305 - top2vec - INFO - Creating joint document/word embedding\n",
            "2021-04-09 02:22:08,502 - top2vec - INFO - Creating lower dimension embedding of documents\n",
            "2021-04-09 02:24:04,008 - top2vec - INFO - Finding dense areas of documents\n",
            "2021-04-09 02:24:11,155 - top2vec - INFO - Finding topics\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX4V_DZ7LSdc",
        "outputId": "65558b98-78a5-487d-c17f-223edb860ba5"
      },
      "source": [
        "model.get_num_topics()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnEiGvlTNYoq",
        "outputId": "4a30c405-8c63-4a7b-c1c2-0e1d46d0a56d"
      },
      "source": [
        "doc_idx = np.arange(0, len(df_train))\n",
        "topic_nums, topic_scores, topic_words, word_scores = model.get_documents_topics(doc_idx)\n",
        "num_prominent_topics = len(np.unique(topic_nums))\n",
        "print(num_prominent_topics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPHGUVrFOD2i"
      },
      "source": [
        "df_train['pred_topics'] = topic_nums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0kzSLNyUyW0"
      },
      "source": [
        "# Topic reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feGNaLRmNn85"
      },
      "source": [
        "hierrarchy = model.hierarchical_topic_reduction(num_topics=NUM_TOPICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n949QuXTOsyq"
      },
      "source": [
        "topic_nums, topic_scores, topic_words, word_scores = get_doc_topics_reduced(df_train, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxMepZgpOwUb"
      },
      "source": [
        "df_train['reduced_topic'] = topic_nums\n",
        "df_train['reduced_topic_score'] = topic_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSpmOTDEO61T",
        "outputId": "539ed88d-b05e-44d8-ffc2-1494a987ad91"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>processed_answer</th>\n",
              "      <th>pred_topics</th>\n",
              "      <th>reduced_topic</th>\n",
              "      <th>reduced_topic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>What makes friendship click?</td>\n",
              "      <td>How does the spark keep going?</td>\n",
              "      <td>good communication is what does it.  Can you m...</td>\n",
              "      <td>good communication move beyond small talk say ...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0.669693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Why does Zebras have stripes?</td>\n",
              "      <td>What is the purpose or those stripes? Who do t...</td>\n",
              "      <td>this provides camouflage - predator vision is ...</td>\n",
              "      <td>provide camouflage predator vision usually dif...</td>\n",
              "      <td>209</td>\n",
              "      <td>6</td>\n",
              "      <td>0.288386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>What did the itsy bitsy sipder climb up?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>waterspout</td>\n",
              "      <td>waterspout</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>0.565577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>What is the difference between a Bachelors and...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One difference between a Bachelors and a Maste...</td>\n",
              "      <td>one difference bachelor master degree requirem...</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0.582758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Why do women get PMS?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Premenstrual syndrome (PMS) is a group of symp...</td>\n",
              "      <td>premenstrual syndrome pm group symptom relate ...</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0.413381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>9</td>\n",
              "      <td>if you could be any fantasy figure, who would ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The invisible man, I'd be straight into the gi...</td>\n",
              "      <td>invisible man straight girl change room</td>\n",
              "      <td>26</td>\n",
              "      <td>4</td>\n",
              "      <td>0.314743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>8</td>\n",
              "      <td>Tell me something about life most people don't...</td>\n",
              "      <td>Do you know anything about life, or words of w...</td>\n",
              "      <td>That there is a hell and everyone thinks their...</td>\n",
              "      <td>hell everyone think go world go dont turn god ...</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>0.498708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>3</td>\n",
              "      <td>Why are men always thinking of sex?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's wired in our brain</td>\n",
              "      <td>wire brain</td>\n",
              "      <td>278</td>\n",
              "      <td>2</td>\n",
              "      <td>0.410009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>6</td>\n",
              "      <td>est ce que DOMENECH est un entraineur: 1: de f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>de foot mais pas pour Ãªtre sÃ©lectionneur d'une...</td>\n",
              "      <td>de foot mais pa pour tre lectionneur une quipe...</td>\n",
              "      <td>25</td>\n",
              "      <td>10</td>\n",
              "      <td>0.503376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>5</td>\n",
              "      <td>No sound or low sound?</td>\n",
              "      <td>I have my volume turned up all the way and hav...</td>\n",
              "      <td>As an old techie, I tend to look at the hardwa...</td>\n",
              "      <td>old techie tend look hardware specifically con...</td>\n",
              "      <td>170</td>\n",
              "      <td>19</td>\n",
              "      <td>0.544819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       topic  ... reduced_topic_score\n",
              "0          9  ...            0.669693\n",
              "1          2  ...            0.288386\n",
              "2          4  ...            0.565577\n",
              "3          4  ...            0.582758\n",
              "4          3  ...            0.413381\n",
              "...      ...  ...                 ...\n",
              "59995      9  ...            0.314743\n",
              "59996      8  ...            0.498708\n",
              "59997      3  ...            0.410009\n",
              "59998      6  ...            0.503376\n",
              "59999      5  ...            0.544819\n",
              "\n",
              "[60000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac6EDZNiP7UC"
      },
      "source": [
        "# Save "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLLd2YXlO9zz"
      },
      "source": [
        "# recommended way to save model\n",
        "model.save(PATH+'/top2vec/saved/'+DATASET+'_to2vec_'+EMBEDDING+'_'+VERSION+'.mdl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEatRcMgRq2R"
      },
      "source": [
        "# another way to save model\n",
        "with open(PATH+'/top2vec/saved/'+DATASET+'_top2vec_'+EMBEDDING+'_'+VERSION+'.model', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL-HWHRGPzRE"
      },
      "source": [
        "# save data\n",
        "with open(PATH+'/top2vec/saved/'+DATASET+'_data'+'_'+VERSION+'.pkl', 'wb') as file:\n",
        "    pickle.dump(df_train, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tRzTQx9qRWY"
      },
      "source": [
        "df_train.to_csv('eq_bank.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}