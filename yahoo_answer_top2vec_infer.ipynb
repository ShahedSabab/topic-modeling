{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yahoo_answer_top2vec_infer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "topic-modeling-env",
      "language": "python",
      "name": "topic-modeling-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eREpKzZGU4-R"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNLayjT-UGbn",
        "outputId": "b8ac21b2-f524-4465-b9dd-3d412757a480"
      },
      "source": [
        "!pip install top2vec[sentence_encoders]\n",
        "!pip install top2vec[sentence_transformers]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: top2vec[sentence_encoders] in /usr/local/lib/python3.7/dist-packages (1.0.24)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.20.2)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.8.27)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.5.0)\n",
            "Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.1.5)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-text; extra == \"sentence_encoders\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (2.4.3)\n",
            "Requirement already satisfied: tensorflow; extra == \"sentence_encoders\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub; extra == \"sentence_encoders\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.11.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.4.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.22)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->top2vec[sentence_encoders]) (7.1.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->top2vec[sentence_encoders]) (5.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_encoders]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_encoders]) (2018.9)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.51.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (54.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.34.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.4.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.10.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (4.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow; extra == \"sentence_encoders\"->top2vec[sentence_encoders]) (3.1.0)\n",
            "Requirement already satisfied: top2vec[sentence_transformers] in /usr/local/lib/python3.7/dist-packages (1.0.24)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (0.5.1)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (0.8.27)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.1.5)\n",
            "Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.20.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.5.0)\n",
            "Requirement already satisfied: torch; extra == \"sentence_transformers\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.8.1+cu101)\n",
            "Requirement already satisfied: sentence-transformers; extra == \"sentence_transformers\" in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_transformers]) (1.0.4)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (1.4.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.5.2)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.51.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.22.2.post1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (0.29.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_transformers]) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_transformers]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_transformers]) (2.8.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->top2vec[sentence_transformers]) (5.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->top2vec[sentence_transformers]) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.7.4.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (0.1.95)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.2.5)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn>=0.5.1->top2vec[sentence_transformers]) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_transformers]) (54.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.10.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (0.0.44)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (0.10.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers; extra == \"sentence_transformers\"->top2vec[sentence_transformers]) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcnAjzB7V1MG"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SMmgdZ9VLNk",
        "outputId": "bd985b81-0230-4cae-d488-4c1e0fcb9d9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhRTtLtAV71j"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPu-2ZPqV3Bo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from top2vec import Top2Vec\n",
        "import re\n",
        "import tensorflow_text\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_RM6UNMWZyE"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnqn5QKGWcZ6"
      },
      "source": [
        "DATASET = 'Dataset-yahoo-answer'\n",
        "PATH = '/content/gdrive/MyDrive/'+DATASET+'/'\n",
        "TOPNWORDS = 20 # topic words\n",
        "EMBEDDING = 'bert'\n",
        "COLUMN_QS = 'paraphrase'\n",
        "COLUMN_ANS = 'parent_id'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAOIMmcAV_Uk"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2fKhTKaV-gJ"
      },
      "source": [
        "with open(PATH+'top2vec/saved/top2vec.model', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "with open(PATH+'top2vec/saved/data.pkl', 'rb') as file:\n",
        "    df = pickle.load(file)\n",
        "\n",
        "topic_name = pd.read_pickle(PATH+'top2vec/saved/dictionary_topic_name.dict')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "L-WOQQjFsw3r",
        "outputId": "acb54af2-63db-4253-c465-e3fafb0aa681"
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>title</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>processed_answer</th>\n",
              "      <th>pred_topics</th>\n",
              "      <th>reduced_topic</th>\n",
              "      <th>reduced_topic_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>What makes friendship click?</td>\n",
              "      <td>How does the spark keep going?</td>\n",
              "      <td>good communication is what does it.  Can you m...</td>\n",
              "      <td>good communication move beyond small talk say ...</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0.542340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Why does Zebras have stripes?</td>\n",
              "      <td>What is the purpose or those stripes? Who do t...</td>\n",
              "      <td>this provides camouflage - predator vision is ...</td>\n",
              "      <td>provide camouflage predator vision usually dif...</td>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>0.258722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>What did the itsy bitsy sipder climb up?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>waterspout</td>\n",
              "      <td>waterspout</td>\n",
              "      <td>56</td>\n",
              "      <td>8</td>\n",
              "      <td>0.403495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>What is the difference between a Bachelors and...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One difference between a Bachelors and a Maste...</td>\n",
              "      <td>one difference bachelor master degree requirem...</td>\n",
              "      <td>24</td>\n",
              "      <td>5</td>\n",
              "      <td>0.503336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Why do women get PMS?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Premenstrual syndrome (PMS) is a group of symp...</td>\n",
              "      <td>premenstrual syndrome pm group symptom relate ...</td>\n",
              "      <td>46</td>\n",
              "      <td>3</td>\n",
              "      <td>0.441360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>9</td>\n",
              "      <td>if you could be any fantasy figure, who would ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The invisible man, I'd be straight into the gi...</td>\n",
              "      <td>invisible man straight girl change room</td>\n",
              "      <td>66</td>\n",
              "      <td>11</td>\n",
              "      <td>0.230436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>8</td>\n",
              "      <td>Tell me something about life most people don't...</td>\n",
              "      <td>Do you know anything about life, or words of w...</td>\n",
              "      <td>That there is a hell and everyone thinks their...</td>\n",
              "      <td>hell everyone think go world go dont turn god ...</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0.462794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>3</td>\n",
              "      <td>Why are men always thinking of sex?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It's wired in our brain</td>\n",
              "      <td>wire brain</td>\n",
              "      <td>19</td>\n",
              "      <td>8</td>\n",
              "      <td>0.495684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>6</td>\n",
              "      <td>est ce que DOMENECH est un entraineur: 1: de f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>de foot mais pas pour être sélectionneur d'une...</td>\n",
              "      <td>de foot mais pa pour tre lectionneur une quipe...</td>\n",
              "      <td>218</td>\n",
              "      <td>6</td>\n",
              "      <td>0.399540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>5</td>\n",
              "      <td>No sound or low sound?</td>\n",
              "      <td>I have my volume turned up all the way and hav...</td>\n",
              "      <td>As an old techie, I tend to look at the hardwa...</td>\n",
              "      <td>old techie tend look hardware specifically con...</td>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>0.438117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       topic  ... reduced_topic_score\n",
              "0          9  ...            0.542340\n",
              "1          2  ...            0.258722\n",
              "2          4  ...            0.403495\n",
              "3          4  ...            0.503336\n",
              "4          3  ...            0.441360\n",
              "...      ...  ...                 ...\n",
              "59995      9  ...            0.230436\n",
              "59996      8  ...            0.462794\n",
              "59997      3  ...            0.495684\n",
              "59998      6  ...            0.399540\n",
              "59999      5  ...            0.438117\n",
              "\n",
              "[60000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScfRhyYNWjBO"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZh0QcX9WiMm"
      },
      "source": [
        "def infer_topics(text, topNwords, num_docs, reduced): \n",
        "    model.add_documents([text])\n",
        "    new_doc_id = len(model.documents) - 1\n",
        "    \n",
        "    if reduced: \n",
        "        result = model.get_documents_topics([new_doc_id], reduced = True)\n",
        "        topic_number = result[0][0]\n",
        "        word_list = model.topic_words_reduced[result[0][0]][:topNwords]\n",
        "        similar_docs = model.search_documents_by_documents([new_doc_id], num_docs)\n",
        "    else:\n",
        "        result = model.get_documents_topics([new_doc_id])\n",
        "        topic_number = result[0][0]\n",
        "        word_list = model.topic_words[result[0][0]][:topNwords]\n",
        "        similar_docs = model.search_documents_by_documents([new_doc_id],num_docs)\n",
        "    \n",
        "    model.delete_documents([new_doc_id])\n",
        "\n",
        "    return word_list, topic_number, similar_docs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VliDUBEG8dS"
      },
      "source": [
        "def add_docs(text):\n",
        "    model.add_documents([text])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AULinIXTZYi"
      },
      "source": [
        "def search_for_topics(keywords, num_topics, reduced):\n",
        "    if reduced:\n",
        "        topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=keywords, num_topics=num_topics, reduced = True)\n",
        "    else:\n",
        "        topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[ \"business\", \"finance\"], num_topics=num_topics)\n",
        "    \n",
        "    for n in topic_nums:\n",
        "        if topic_name[n] != '_':\n",
        "            print(topic_name[n])\n",
        "            break\n",
        "    \n",
        "    return topic_words, word_scores, topic_scores, topic_nums"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JSkwkaaTZYj"
      },
      "source": [
        "def search_for_documents_by_keywords(keywords, num_docs):\n",
        "    # this does not have any reduced parameter\n",
        "    documents, document_scores, document_ids = model.search_documents_by_keywords(keywords=keywords, num_docs=num_docs)\n",
        "    for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
        "        print(f\"Document: {doc_id}, Score: {score}\")\n",
        "        print(\"-----------\")\n",
        "        print(doc)\n",
        "        print(\"-----------\")\n",
        "        print()\n",
        "    return documents, document_scores, document_ids"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79XBm_41TZYj"
      },
      "source": [
        "def search_for_keywords(keywords_pos, keywords_neg, num_words):\n",
        "    # this does not have any reduced parameter\n",
        "    words, word_scores = model.similar_words(keywords=keywords_pos, keywords_neg=keywords_neg, num_words=num_words)\n",
        "    for word, score in zip(words, word_scores):\n",
        "        print(f\"{word} {score}\")\n",
        "    return words, word_scores"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ZP8MB3TZYj"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMNkyo20WPTj"
      },
      "source": [
        "text = r'''\n",
        "Ontario reported 4,156 new cases of COVID-19 and the deaths of 28 more people with the illness on Wednesday, while public health units administered a new record-high number of vaccine doses.\n",
        "'''"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCdQaoHvZMg3"
      },
      "source": [
        "word_list, topic_number, similar_document = infer_topics(text, topNwords = TOPNWORDS, num_docs = 10, reduced = True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SN6DAQsltgVb",
        "outputId": "83a83824-2fbd-4d3b-f6f1-502b872293eb"
      },
      "source": [
        "topic_name[topic_number]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'health'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiD9GdvZTZYk"
      },
      "source": [
        "## Search for Topics by Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aODq-JcUTZYk",
        "outputId": "29c24b35-02c7-4a70-a7e6-a678f6c87d1b"
      },
      "source": [
        "topic_words, word_scores, topic_scores, topic_nums = search_for_topics(keywords = ['finance', 'stock', 'money'],num_topics = 10, reduced = True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGKAfiYrTZYk"
      },
      "source": [
        "## Search for related Documents by Keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRxHifOSTZYl",
        "outputId": "153581f2-7289-4cb5-afeb-8f5991b126a7"
      },
      "source": [
        "documents, document_scores, document_ids = search_for_documents_by_keywords(keywords = ['business', 'diabetes'], num_docs = 5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document: 23392, Score: 0.5093256331546931\n",
            "-----------\n",
            "life\n",
            "-----------\n",
            "\n",
            "Document: 47558, Score: 0.5025011939387787\n",
            "-----------\n",
            "find diabetic\n",
            "-----------\n",
            "\n",
            "Document: 8254, Score: 0.4907497425179201\n",
            "-----------\n",
            "professional\n",
            "-----------\n",
            "\n",
            "Document: 16613, Score: 0.48704823216284576\n",
            "-----------\n",
            "health problem\n",
            "-----------\n",
            "\n",
            "Document: 1499, Score: 0.47717358684168376\n",
            "-----------\n",
            "money\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q7yjEhsWTZYl",
        "outputId": "1ccc6edc-f6c3-4fa8-9c1f-fcc379ddff11"
      },
      "source": [
        "df.iloc[59177]['answer']"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'If it is a psychiatric hospital it is!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT2rt5cNTZYl"
      },
      "source": [
        "# Search for similar keywords by keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rObBiU-iTZYl",
        "outputId": "8d3bfaca-b05a-46e4-8679-44fa9081d90b"
      },
      "source": [
        "words, word_scores = search_for_keywords(keywords_pos = ['health'], keywords_neg=None, num_words=10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "healthy 0.6893834201415286\n",
            "fitness 0.582503345940204\n",
            "medical 0.5744456364324823\n",
            "illness 0.5594899883472886\n",
            "disease 0.5336805571374086\n",
            "nutrition 0.5271034051463707\n",
            "care 0.5244941710352499\n",
            "lifestyle 0.5131066954114256\n",
            "business 0.511849214834601\n",
            "food 0.5035717969279957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVz6E1ybTZYl"
      },
      "source": [
        "# Search for similar documents by documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbA-Fi7lTZYm"
      },
      "source": [
        "document_text = '''\n",
        "what is computer\n",
        "'''"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCQBu5QXTZYm"
      },
      "source": [
        "word_list, topic_number, similar_document = infer_topics(document_text, topNwords = 10, num_docs = 10, reduced = True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lQqe9hdTZYm",
        "outputId": "8ccdc03a-7939-4ae1-f738-a749a00efd90"
      },
      "source": [
        "similar_document"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['whats', 'whats', 'pcworld com computer', 'stand alone computer',\n",
              "        'whats question', 'vedeos whats', 'dont add computer calculater',\n",
              "        'describe homework',\n",
              "        'depend define word computer succession steadily powerful flexible compute device construct gradually add key feature modern computer use digital electronics invent claude shannon flexible programmability define one point along road first computer exceedingly difficult notable achievement include atanasoff berry computer special purpose machine use valve drive vacuum tube computation binary number regenerative memory secret british colossus computer limit programmability demonstrate device use thousand valve could make reliable reprogrammed electronically american eniac one first general purpose machine still use decimal system incorporate inflexible architecture mean reprogramming essentially require rewire konrad zuse z machine electromechanical z first work machine feature automatic binary arithmetic feasible programability',\n",
              "        'c est l amour quoi'], dtype=object),\n",
              " array([0.46307414, 0.46307414, 0.45326414, 0.44314572, 0.43621047,\n",
              "        0.4253305 , 0.41100864, 0.39875044, 0.39649175, 0.39278813]),\n",
              " array([25627, 20996, 41031, 44581,  6038, 17653, 38523, 16013,  2795,\n",
              "        40380]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyxBeJT_TZYm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKwuD-vyTZYm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}